{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAgA+11oMV9CXTjIC9UJmB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juuuuls/CPE311/blob/main/HoA_8_1_Supplementary_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activity:** Hands-On-Activity_8.1\n",
        "\n",
        "**Name:** Quibral, Juliann Vincent B.\n",
        "\n",
        "**Section:** BSCPE22S3"
      ],
      "metadata": {
        "id": "f0umEMfPt33I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. With the earthquakes.csv file, select all the earthquakes in Japan with a magType of mb and a magnitude of 4.9 or greater."
      ],
      "metadata": {
        "id": "kE9FS5iiqx02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr0xw7nup5sk",
        "outputId": "ada1ed6d-103d-4b1c-c5e4-bf8dd900614a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      mag magType           time                         place  tsunami  \\\n",
            "1563  4.9      mb  1538977532250  293km ESE of Iwo Jima, Japan        0   \n",
            "2576  5.4      mb  1538697528010    37km E of Tomakomai, Japan        0   \n",
            "3072  4.9      mb  1538579732490     15km ENE of Hasaki, Japan        0   \n",
            "3632  4.9      mb  1538450871260    53km ESE of Hitachi, Japan        0   \n",
            "\n",
            "     parsed_place  \n",
            "1563        Japan  \n",
            "2576        Japan  \n",
            "3072        Japan  \n",
            "3632        Japan  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"earthquakes.csv\")\n",
        "\n",
        "japan_earthquakes = df[(df['parsed_place'] == 'Japan') & (df['magType'] == 'mb') & (df['mag'] >= 4.9)]\n",
        "\n",
        "print(japan_earthquakes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create bins for each full number of magnitude (for example, the first bin is 0-1, the second is 1-2, and so on) with a magType of ml and count how many are in each bin."
      ],
      "metadata": {
        "id": "wpf-Xlj2rAoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"earthquakes.csv\")\n",
        "\n",
        "# Filter earthquakes with magType 'ml'\n",
        "ml_earthquakes = df[df['magType'] == 'ml']\n",
        "\n",
        "# Create bins for each full number of magnitude\n",
        "bins = [i for i in range(11)]\n",
        "\n",
        "# Count earthquakes in each bin\n",
        "magnitude_counts = pd.cut(ml_earthquakes['mag'], bins=bins, right=False).value_counts().sort_index()\n",
        "\n",
        "print(magnitude_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TODrelZQrD6b",
        "outputId": "9055261d-300e-4296-cb79-cb79521e071f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1)     2072\n",
            "[1, 2)     3126\n",
            "[2, 3)      985\n",
            "[3, 4)      153\n",
            "[4, 5)        6\n",
            "[5, 6)        2\n",
            "[6, 7)        0\n",
            "[7, 8)        0\n",
            "[8, 9)        0\n",
            "[9, 10)       0\n",
            "Name: mag, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Using the faang.csv file, group by the ticker and resample to monthly frequency. Make the following aggregations:\n",
        "\n",
        "*   Mean of the opening price\n",
        "*   Maximum of the high price\n",
        "*   Minimum of the low price\n",
        "*   Mean of the closing price\n",
        "*   Sum of the volume traded\n"
      ],
      "metadata": {
        "id": "kAsh6m8irIDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"faang.csv\", index_col='date', parse_dates=True) # Read the CSV file into a pandas DataFrame\n",
        "\n",
        "monthly_aggregations = df.groupby('ticker').resample('M').agg({ # resample to monthly frequency\n",
        "    'open': 'mean',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'mean',\n",
        "    'volume': 'sum'\n",
        "})\n",
        "\n",
        "print(monthly_aggregations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1eumlsvrIdI",
        "outputId": "c08a0a01-0ced-4bf6-f25a-960a0d902a90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          open       high        low        close     volume\n",
            "ticker date                                                                 \n",
            "AAPL   2018-01-31   170.714690   176.6782   161.5708   170.699271  659679440\n",
            "       2018-02-28   164.562753   177.9059   147.9865   164.921884  927894473\n",
            "       2018-03-31   172.421381   180.7477   162.4660   171.878919  713727447\n",
            "       2018-04-30   167.332895   176.2526   158.2207   167.286924  666360147\n",
            "       2018-05-31   182.635582   187.9311   162.7911   183.207418  620976206\n",
            "       2018-06-30   186.605843   192.0247   178.7056   186.508652  527624365\n",
            "       2018-07-31   188.065786   193.7650   181.3655   188.179724  393843881\n",
            "       2018-08-31   210.460287   227.1001   195.0999   211.477743  700318837\n",
            "       2018-09-30   220.611742   227.8939   213.6351   220.356353  678972040\n",
            "       2018-10-31   219.489426   231.6645   204.4963   219.137822  789748068\n",
            "       2018-11-30   190.828681   220.6405   169.5328   190.246652  961321947\n",
            "       2018-12-31   164.537405   184.1501   145.9639   163.564732  898917007\n",
            "AMZN   2018-01-31  1301.377143  1472.5800  1170.5100  1309.010952   96371290\n",
            "       2018-02-28  1447.112632  1528.7000  1265.9300  1442.363158  137784020\n",
            "       2018-03-31  1542.160476  1617.5400  1365.2000  1540.367619  130400151\n",
            "       2018-04-30  1475.841905  1638.1000  1352.8800  1468.220476  129945743\n",
            "       2018-05-31  1590.474545  1635.0000  1546.0200  1594.903636   71615299\n",
            "       2018-06-30  1699.088571  1763.1000  1635.0900  1698.823810   85941510\n",
            "       2018-07-31  1786.305714  1880.0500  1678.0600  1784.649048   97629820\n",
            "       2018-08-31  1891.957826  2025.5700  1776.0200  1897.851304   96575676\n",
            "       2018-09-30  1969.239474  2050.5000  1865.0000  1966.077895   94445693\n",
            "       2018-10-31  1799.630870  2033.1900  1476.3600  1782.058261  183228552\n",
            "       2018-11-30  1622.323810  1784.0000  1420.0000  1625.483810  139290208\n",
            "       2018-12-31  1572.922105  1778.3400  1307.0000  1559.443158  154812304\n",
            "FB     2018-01-31   184.364762   190.6600   175.8000   184.962857  495655736\n",
            "       2018-02-28   180.721579   195.3200   167.1800   180.269474  516621991\n",
            "       2018-03-31   173.449524   186.1000   149.0200   173.489524  996232472\n",
            "       2018-04-30   164.163557   177.1000   150.5100   163.810476  751130388\n",
            "       2018-05-31   181.910509   192.7200   170.2300   182.930000  401144183\n",
            "       2018-06-30   194.974067   203.5500   186.4300   195.267619  387265765\n",
            "       2018-07-31   199.332143   218.6200   166.5600   199.967143  652763259\n",
            "       2018-08-31   177.598443   188.3000   170.2700   177.491957  549016789\n",
            "       2018-09-30   164.232895   173.8900   158.8656   164.377368  500468912\n",
            "       2018-10-31   154.873261   165.8800   139.0300   154.187826  622446235\n",
            "       2018-11-30   141.762857   154.1300   126.8500   141.635714  518150415\n",
            "       2018-12-31   137.529474   147.1900   123.0200   137.161053  558786249\n",
            "GOOG   2018-01-31  1127.200952  1186.8900  1045.2300  1130.770476   28738485\n",
            "       2018-02-28  1088.629474  1174.0000   992.5600  1088.206842   42384105\n",
            "       2018-03-31  1096.108095  1177.0500   980.6400  1091.490476   45430049\n",
            "       2018-04-30  1038.415238  1094.1600   990.3700  1035.696190   41773275\n",
            "       2018-05-31  1064.021364  1110.7500  1006.2900  1069.275909   31849196\n",
            "       2018-06-30  1136.396190  1186.2900  1096.0100  1137.626667   32103642\n",
            "       2018-07-31  1183.464286  1273.8900  1093.8000  1187.590476   31953386\n",
            "       2018-08-31  1226.156957  1256.5000  1188.2400  1225.671739   28820379\n",
            "       2018-09-30  1176.878421  1212.9900  1146.9100  1175.808947   28863199\n",
            "       2018-10-31  1116.082174  1209.9600   995.8300  1110.940435   48496167\n",
            "       2018-11-30  1054.971429  1095.5700   996.0200  1056.162381   36735570\n",
            "       2018-12-31  1042.620000  1124.6500   970.1100  1037.420526   40256461\n",
            "NFLX   2018-01-31   231.269286   286.8100   195.4200   232.908095  238377533\n",
            "       2018-02-28   270.873158   297.3600   236.1100   271.443684  184585819\n",
            "       2018-03-31   312.712857   333.9800   275.9000   312.228095  263449491\n",
            "       2018-04-30   309.129529   338.8200   271.2239   307.466190  262064417\n",
            "       2018-05-31   329.779759   356.1000   305.7300   331.536818  142051114\n",
            "       2018-06-30   384.557595   423.2056   352.8200   384.133333  244032001\n",
            "       2018-07-31   380.969090   419.7700   328.0000   381.515238  305487432\n",
            "       2018-08-31   345.409591   376.8085   310.9280   346.257826  213144082\n",
            "       2018-09-30   363.326842   383.2000   335.8300   362.641579  170832156\n",
            "       2018-10-31   340.025348   386.7999   271.2093   335.445652  363589920\n",
            "       2018-11-30   290.643333   332.0499   250.0000   290.344762  257126498\n",
            "       2018-12-31   266.309474   298.7200   231.2300   265.302368  234304628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Build a crosstab with the earthquake data between the tsunami column and the magType column. Rather than showing the frequency count, show the maximum\n",
        "magnitude that was observed for each combination. Put the magType along the columns."
      ],
      "metadata": {
        "id": "wAHCuZW0rIjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"earthquakes.csv\")\n",
        "\n",
        "crosstab_max_magnitude = pd.crosstab(df['tsunami'], df['magType'], values=df['mag'], aggfunc='max') # Create the crosstab with max magnitude observed for each combination\n",
        "\n",
        "print(crosstab_max_magnitude)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY8SX27ArIp9",
        "outputId": "fde24cad-348f-4c4d-b728-5bd1df4e8390"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "magType   mb  mb_lg    md   mh   ml  ms_20    mw  mwb  mwr  mww\n",
            "tsunami                                                        \n",
            "0        5.6    3.5  4.11  1.1  4.2    NaN  3.83  5.8  4.8  6.0\n",
            "1        6.1    NaN   NaN  NaN  5.1    5.7  4.41  NaN  NaN  7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Calculate the rolling 60-day aggregations of OHLC data by ticker for the FAANG data. Use the same aggregations as exercise no. 3.\n"
      ],
      "metadata": {
        "id": "u3s1Dvy9rpA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"faang.csv\", index_col='date', parse_dates=True)\n",
        "\n",
        "aggregations = {\n",
        "    'open': 'mean',\n",
        "    'high': 'max',\n",
        "    'low': 'min',\n",
        "    'close': 'mean',\n",
        "    'volume': 'sum'\n",
        "}\n",
        "\n",
        "\n",
        "rolling_60_day_aggregations = df.groupby('ticker').rolling(window='60D').agg(aggregations) # Calculate rolling 60-day aggregations by ticker\n",
        "\n",
        "print(rolling_60_day_aggregations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSdAotqjrnxa",
        "outputId": "fc20054f-71a4-48f1-dec3-b13b9d8a323c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         open      high       low       close       volume\n",
            "ticker date                                                               \n",
            "AAPL   2018-01-02  166.927100  169.0264  166.0442  168.987200   25555934.0\n",
            "       2018-01-03  168.089600  171.2337  166.0442  168.972500   55073833.0\n",
            "       2018-01-04  168.480367  171.2337  166.0442  169.229200   77508430.0\n",
            "       2018-01-05  168.896475  172.0381  166.0442  169.840675  101168448.0\n",
            "       2018-01-08  169.324680  172.2736  166.0442  170.080040  121736214.0\n",
            "...                       ...       ...       ...         ...          ...\n",
            "NFLX   2018-12-24  283.509250  332.0499  233.6800  281.931750  525657894.0\n",
            "       2018-12-26  281.844500  332.0499  231.2300  280.777750  520444588.0\n",
            "       2018-12-27  281.070488  332.0499  231.2300  280.162805  532679805.0\n",
            "       2018-12-28  279.916341  332.0499  231.2300  279.461341  521968250.0\n",
            "       2018-12-31  278.430769  332.0499  231.2300  277.451410  476309676.0\n",
            "\n",
            "[1255 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a pivot table of the FAANG data that compares the stocks. Put the ticker in the rows and show the averages of the OHLC and volume traded data."
      ],
      "metadata": {
        "id": "PhGmD8kXrwmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"faang.csv\", index_col='date', parse_dates=True)\n",
        "\n",
        "columns = ['open', 'high', 'low', 'close', 'volume'] # Define the columns for which we want to calculate the averages\n",
        "\n",
        "pivot_table_faang = df.pivot_table(index='ticker', values=columns, aggfunc='mean')# Create the pivot table\n",
        "\n",
        "print(pivot_table_faang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRjRconnrw2A",
        "outputId": "0bf12bd5-c696-420e-e0b8-1585eef0711b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              close         high          low         open        volume\n",
            "ticker                                                                  \n",
            "AAPL     186.986218   188.906858   185.135729   187.038674  3.402145e+07\n",
            "AMZN    1641.726175  1662.839801  1619.840398  1644.072669  5.649563e+06\n",
            "FB       171.510936   173.615298   169.303110   171.454424  2.768798e+07\n",
            "GOOG    1113.225139  1125.777649  1101.001594  1113.554104  1.742645e+06\n",
            "NFLX     319.290299   325.224583   313.187273   319.620533  1.147030e+07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Calculate the Z-scores for each numeric column of Netflix's data (ticker is NFLX) using apply()."
      ],
      "metadata": {
        "id": "Dvedp729sAUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "df = pd.read_csv(\"faang.csv\", index_col='date', parse_dates=True)\n",
        "\n",
        "netflix_data = df[df['ticker'] == 'NFLX'] # Filter data for Netflix\n",
        "\n",
        "numeric_columns = netflix_data.select_dtypes(include='number') # Select only numeric columns\n",
        "\n",
        "z_scores = numeric_columns.apply(zscore) # Calculate Z-scores for each numeric column\n",
        "\n",
        "print(z_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T-cJs6lsAzi",
        "outputId": "d275fcc4-9716-4c02-b511-423932e693df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                open      high       low     close    volume\n",
            "date                                                        \n",
            "2018-01-02 -2.505749 -2.521050 -2.415042 -2.421473 -0.088937\n",
            "2018-01-03 -2.385047 -2.428022 -2.290360 -2.339951 -0.508620\n",
            "2018-01-04 -2.300860 -2.410885 -2.239081 -2.328071 -0.961204\n",
            "2018-01-05 -2.279559 -2.350294 -2.206487 -2.238767 -0.783894\n",
            "2018-01-08 -2.223367 -2.299699 -2.148042 -2.196572 -1.040606\n",
            "...              ...       ...       ...       ...       ...\n",
            "2018-12-24 -1.574618 -1.521399 -1.630448 -1.749435 -0.339680\n",
            "2018-12-26 -1.738529 -1.442855 -1.680690 -1.344082  0.518073\n",
            "2018-12-27 -1.410097 -1.420618 -1.498794 -1.305267  0.135138\n",
            "2018-12-28 -1.251257 -1.291594 -1.299877 -1.294718 -0.085334\n",
            "2018-12-31 -1.206222 -1.124597 -1.090706 -1.057529  0.360163\n",
            "\n",
            "[251 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Add event descriptions:\n",
        "\n",
        "* Create a dataframe with the following three columns: ticker, date, and event. The columns should have the following values:\n",
        "* ticker: 'FB'\n",
        "* date: ['2018-07-25', '2018-03-19', '2018-03-20']\n",
        "* event: ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']\n",
        "* Set the index to ['date', 'ticker']\n",
        "* Merge this data with the FAANG data using an outer join\n"
      ],
      "metadata": {
        "id": "CZUdARO5sKPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "event_data = {\n",
        "    'ticker': ['FB', 'FB', 'FB'],\n",
        "    'date': ['2018-07-25', '2018-03-19', '2018-03-20'],\n",
        "    'event': ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']\n",
        "}\n",
        "events_df = pd.DataFrame(event_data)\n",
        "\n",
        "events_df['date'] = pd.to_datetime(events_df['date']) # Convert date column to datetime type\n",
        "\n",
        "events_df.drop_duplicates(inplace=True) # Remove duplicates\n",
        "\n",
        "events_df.set_index(['date', 'ticker'], inplace=True)\n",
        "\n",
        "faang_df = pd.read_csv(\"faang.csv\", parse_dates=['date'])# Read the FAANG data into a pandas DataFrame and reset index\n",
        "faang_df.set_index(['date', 'ticker'], inplace=True)\n",
        "\n",
        "merged_df = faang_df.merge(events_df, how='left', left_index=True, right_index=True) # Merge the event data with the FAANG data using a left join\n",
        "\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCYtf9HRsKa1",
        "outputId": "ea5a6795-92ff-4a2b-e293-6cd5fc21a671"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      open     high        low    close    volume event\n",
            "date       ticker                                                      \n",
            "2018-01-02 FB       177.68   181.58   177.5500   181.42  18151903   NaN\n",
            "2018-01-03 FB       181.88   184.78   181.3300   184.67  16886563   NaN\n",
            "2018-01-04 FB       184.90   186.21   184.0996   184.33  13880896   NaN\n",
            "2018-01-05 FB       185.59   186.90   184.9300   186.85  13574535   NaN\n",
            "2018-01-08 FB       187.20   188.90   186.3300   188.28  17994726   NaN\n",
            "...                    ...      ...        ...      ...       ...   ...\n",
            "2018-12-24 GOOG     973.90  1003.54   970.1100   976.22   1590328   NaN\n",
            "2018-12-26 GOOG     989.01  1040.00   983.0000  1039.46   2373270   NaN\n",
            "2018-12-27 GOOG    1017.15  1043.89   997.0000  1043.88   2109777   NaN\n",
            "2018-12-28 GOOG    1049.62  1055.56  1033.1000  1037.08   1413772   NaN\n",
            "2018-12-31 GOOG    1050.96  1052.70  1023.5900  1035.61   1493722   NaN\n",
            "\n",
            "[1255 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Use the transform() method on the FAANG data to represent all the values in terms of the first date in the data. To do so, divide all the values for each ticker by the values\n",
        "for the first date in the data for that ticker. This is referred to as an index, and the data for the first date is the base (https://ec.europa.eu/eurostat/statistics-explained/\n",
        "index.php/ Beginners:Statisticalconcept-Indexandbaseyear). When data is in this format, we can easily see growth over time. Hint: transform() can take a function name.\n"
      ],
      "metadata": {
        "id": "aZgzGkhLslC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "faang_df = pd.read_csv(\"faang.csv\", parse_dates=['date'])\n",
        "\n",
        "def calculate_index(x):\n",
        "    return x / x.iloc[0]\n",
        "\n",
        "indexed_faang_df = faang_df.groupby('ticker').transform(calculate_index) # Group the data by ticker and apply the custom function using transform()\n",
        "\n",
        "print(indexed_faang_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBWCB_fHsllR",
        "outputId": "b91f2aef-f01e-48f4-c28c-7b7714f670f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          open      high       low     close    volume\n",
            "0     1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "1     1.023638  1.017623  1.021290  1.017914  0.930292\n",
            "2     1.040635  1.025498  1.036889  1.016040  0.764707\n",
            "3     1.044518  1.029298  1.041566  1.029931  0.747830\n",
            "4     1.053579  1.040313  1.049451  1.037813  0.991341\n",
            "...        ...       ...       ...       ...       ...\n",
            "1250  0.928993  0.940578  0.928131  0.916638  1.285047\n",
            "1251  0.943406  0.974750  0.940463  0.976019  1.917695\n",
            "1252  0.970248  0.978396  0.953857  0.980169  1.704782\n",
            "1253  1.001221  0.989334  0.988395  0.973784  1.142383\n",
            "1254  1.002499  0.986653  0.979296  0.972404  1.206986\n",
            "\n",
            "[1255 rows x 5 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-9f587ecc3e0a>:8: FutureWarning: Dropping invalid columns in DataFrameGroupBy.transform is deprecated. In a future version, a TypeError will be raised. Before calling .transform, select only columns which should be valid for the function.\n",
            "  indexed_faang_df = faang_df.groupby('ticker').transform(calculate_index) # Group the data by ticker and apply the custom function using transform()\n"
          ]
        }
      ]
    }
  ]
}